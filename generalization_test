# =========================================================
# Fine-tuning MULTICLASS ViT on PBC
# (consistent with internal ViT training)
# =========================================================

import os
import random
import numpy as np
from PIL import Image
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import timm
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt

# -------------------------
# 0) Config
# -------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED = 42

PBC_PATH = "/lapix/pbc_dataset/PBC_dataset_split/PBC_dataset_split"
MODEL_WEIGHTS_PATH = "vit_base_best.pth"   # modelo treinado no dataset privado
OUTPUT_BEST_MODEL = "vit_pbc_finetuned_best.pth"

BATCH_SIZE = 16
LR = 1e-5
WEIGHT_DECAY = 1e-2
EPOCHS = 50
PATIENCE = 7
NUM_WORKERS = 4
IMG_SIZE = 384

# -------------------------
# 1) Seed
# -------------------------
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# -------------------------
# 2) Classes (ordem EXATA do treino original)
# -------------------------
MODEL_CLASS_NAMES = [
    "Artefato",
    "Basofilo",
    "Bastonete",
    "Blasto",
    "Eosinofilo",
    "Eritroblasto",
    "Linfocito",
    "Linfocito atipico",
    "Metamielocito",
    "Mielocito",
    "Monocito",
    "Neutrofilo segmentado",
    "Promielocito",
    "Restos celulares"
]

NUM_CLASSES = len(MODEL_CLASS_NAMES)

# -------------------------
# 3) Mapeamento PBC → classes do modelo
# -------------------------
PBC_TO_MODEL = {
    "basophil": "Basofilo",
    "eosinophil": "Eosinofilo",
    "erythroblast": "Eritroblasto",
    "lymphocyte": "Linfocito",
    "monocyte": "Monocito",
    "neutrophil": "Neutrofilo segmentado"
}

# -------------------------
# 4) Dataset (MULTICLASS)
# -------------------------
class PBCDataset(Dataset):
    def __init__(self, root_dir, split="Train", transform=None):
        self.root = os.path.join(root_dir, split)
        self.transform = transform
        self.samples = []

        for pbc_cls in sorted(os.listdir(self.root)):
            folder = os.path.join(self.root, pbc_cls)
            if not os.path.isdir(folder):
                continue

            if pbc_cls not in PBC_TO_MODEL:
                continue

            model_label = PBC_TO_MODEL[pbc_cls]
            label_idx = MODEL_CLASS_NAMES.index(model_label)

            for fname in os.listdir(folder):
                if fname.startswith("."):
                    continue
                self.samples.append(
                    (os.path.join(folder, fname), label_idx)
                )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        image = Image.open(path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, label

# -------------------------
# 5) Transforms
# -------------------------
train_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

val_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

train_dataset = PBCDataset(PBC_PATH, split="Train", transform=train_tf)
val_dataset   = PBCDataset(PBC_PATH, split="Val",   transform=val_tf)

train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE,
    shuffle=True, num_workers=NUM_WORKERS
)

val_loader = DataLoader(
    val_dataset, batch_size=BATCH_SIZE,
    shuffle=False, num_workers=NUM_WORKERS
)

print(f"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}")

# -------------------------
# 6) Model
# -------------------------
model = timm.create_model(
    "vit_base_patch16_384",
    pretrained=False,
    num_classes=NUM_CLASSES
)

state = torch.load(MODEL_WEIGHTS_PATH, map_location="cpu")
model.load_state_dict(state)
model.to(DEVICE)

# Freeze backbone (safe fine-tuning)
for name, p in model.named_parameters():
    if "head" not in name:
        p.requires_grad = False

# -------------------------
# 7) Loss / Optimizer
# -------------------------
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=LR, weight_decay=WEIGHT_DECAY
)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", patience=3, factor=0.1
)

# -------------------------
# 8) Training loop
# -------------------------
best_val_loss = float("inf")
patience_counter = 0

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0

    for imgs, labels in train_loader:
        imgs = imgs.to(DEVICE)
        labels = labels.to(DEVICE)

        optimizer.zero_grad()
        logits = model(imgs)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * imgs.size(0)

    train_loss /= len(train_loader.dataset)

    # -------- Validation --------
    model.eval()
    val_loss = 0.0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs = imgs.to(DEVICE)
            labels = labels.to(DEVICE)

            logits = model(imgs)
            loss = criterion(logits, labels)

            val_loss += loss.item() * imgs.size(0)

            preds = logits.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss /= len(val_loader.dataset)
    scheduler.step(val_loss)

    acc = accuracy_score(all_labels, all_preds)
    prec, rec, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average="macro", zero_division=0
    )

    print(
        f"Epoch {epoch+1:02d} | "
        f"Train Loss: {train_loss:.4f} | "
        f"Val Loss: {val_loss:.4f} | "
        f"Acc: {acc:.4f} | F1-macro: {f1:.4f}"
    )

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), OUTPUT_BEST_MODEL)
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("Early stopping triggered.")
            break

# -------------------------
# 9) Final evaluation + confusion matrix
# -------------------------
model.load_state_dict(torch.load(OUTPUT_BEST_MODEL))
model.eval()

all_preds, all_labels = [], []

with torch.no_grad():
    for imgs, labels in val_loader:
        imgs = imgs.to(DEVICE)
        logits = model(imgs)
        preds = logits.argmax(dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(
    cm,
    display_labels=[MODEL_CLASS_NAMES[i] for i in sorted(set(all_labels))]
)
disp.plot(xticks_rotation=45, cmap="Blues")
plt.tight_layout()
plt.show()

print("✅ Fine-tuning multiclass no PBC concluído.")
